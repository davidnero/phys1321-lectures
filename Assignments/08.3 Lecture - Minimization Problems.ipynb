{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fill in any place that says `# YOUR CODE HERE` or YOUR ANSWER HERE, as well as your name and collaborators below.\n",
    "Grading for pre-lecture assignments is all or nothing. Partial credit is available for in-class assignments and checkpoints, but **only when code is commented**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c08f0f190fa275e6907067afd2e4b9ad",
     "grade": false,
     "grade_id": "cell-ec0c8f83ffb0d9c7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Learning Objectives\n",
    "\n",
    "This lecture will show you how to:\n",
    "1. Implement golden ratio search to find a function's local minimum\n",
    "2. Find a minimum with the gradient descent method\n",
    "3. Use `scipy.optimize.minimize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14e86d9252116059b4fd131609ee0fcd",
     "grade": false,
     "grade_id": "cell-abd1b2cca923116d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "\n",
    "import grading_helper as _test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9db9a06d5acfd63fefc555492f94aff",
     "grade": false,
     "grade_id": "cell-12b43295a0f000dd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Golden Ratio Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52131bb39775730cae3d56d657713175",
     "grade": false,
     "grade_id": "cell-e9692b2c91e495dc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%video B97KuhRP5gM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e4204def0c29e9d8cdaf8726ec9329d",
     "grade": false,
     "grade_id": "cell-ef2de58511545032",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Summary:\n",
    "\n",
    "- A common calculation is to find the minimum/maximum of a function. (The max of $f$ is the same as the min of $-f$.)\n",
    "- Regardless of which method we choose, we will find a **local minimum**, which might not be the same as the **global minimum**.\n",
    "- There are many numerical algorithms to pick from, but we'll start with a simple one called **golden ratio search** that shares a lot of similarities with the binary search method.\n",
    "- Recipe:\n",
    "    1. Start with points $x_1$ and $x_4$ that bracket the minimum. The size of the interval is $\\Delta x = x_4 - x_1$.\n",
    "    2. Calculate two intermediate points:\n",
    "    $$x_2 = x_4 - \\frac{\\Delta x}{\\phi}\\qquad x_3 = x_1 + \\frac{\\Delta x}{\\phi}\\,,$$\n",
    "    where $\\phi=\\tfrac12(1+\\sqrt{5})=1.618\\ldots$ is the [golden ratio](https://en.wikipedia.org/wiki/Golden_ratio).\n",
    "    3. Check to see which of $f(x_2)$ and $f(x_3)$ is smallest. If $f(x_2)$ is smallest, then redefine\n",
    "    $x_4=x_3$, $x_3=x_2$, and generate a new $x_2$. Otherwise, $x_1=x_2$, $x_2=x_3$, and generate a new $x_3$.\n",
    "    4. Repeat until $\\Delta x$ has reached your tolerance.\n",
    "- `scipy.optimize.minimize_scalar(f, bracket=(x1, x4))` implements this method with a few enhancements that speed up convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23cb1118dec8a496919a1f6d220484be",
     "grade": false,
     "grade_id": "cell-e97e11861c0dcf51",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Your Turn\n",
    "\n",
    "The Buckingham potential approximates the potential energy of two neighboring atoms in a solid.\n",
    "\n",
    "$$V(r) = V_0\\left[\\left(\\frac{\\sigma}{r}\\right)^6-e^{-r/\\sigma}\\right].$$\n",
    "\n",
    "The minimum of this function is the typical distance between atoms. In units where $\\sigma = 1$, use `optimize.minimize_scalar` to find the value of $r$ that minimize this function. Store the result in a varibale named `r`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9636558184ba63bff40e075a218fab7",
     "grade": false,
     "grade_id": "cell-35ad44336baecb36",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%%graded # 2 points\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d322b2daf007de24b0870ca98bb4222d",
     "grade": true,
     "grade_id": "cell-da8b5d94fab34508",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%%tests\n",
    "\n",
    "_test.code_contains(\"minimize_scalar\")\n",
    "_test.similar(r, 1.63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e9db7dea23601bf8f176c8a72367522",
     "grade": false,
     "grade_id": "cell-c3979e64ecb0f78e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a5dc718ee7244c73a7ef78496d6791e",
     "grade": false,
     "grade_id": "cell-e55d933eef813232",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%video vjmHD2g-YmY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73feaca9b4f6fa49d0f92af49a59c7c3",
     "grade": false,
     "grade_id": "cell-3fc428fa4e7fa976",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Summary:\n",
    "\n",
    "- Another popular family of minimizers is based on the **gradient decent**. They have a lot of similarities with the Newton-Raphson method.\n",
    "- In brief, we iterate on\n",
    "$$\\mathbf{x}=\\mathbf{x}-\\gamma\\,\\vec{\\nabla} f(\\mathbf{x})\\,,$$\n",
    "where $\\mathbf{x}$ is a vector of values. The art lies in picking a good value for the constant $\\gamma$. Too small, and it takes a long time to converge. Too large, and it will keep overshooting the solution, and never converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fe79f5ada97ba24a44a7ee42d5a39746",
     "grade": false,
     "grade_id": "cell-816be27c799e9a5e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Using `scipy.optimize.minimize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64b59f9bb16e10e5652580e8601822a2",
     "grade": false,
     "grade_id": "cell-938a3475f0b78917",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%video wS5D72wLez8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d41e57dd7612a81311634163e5ef3b2e",
     "grade": false,
     "grade_id": "cell-3b9c0f7fca8fa09f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Summary:\n",
    "\n",
    "- Use `scipy.optimize.minimize(f, guess)` as a wrapper around 14 different minimization routines. The default routine is in the gradient descent family.\n",
    "- Also of note is `method=\"Powell\"` which incorporates ideas from the golden ratio search.\n",
    "- See the [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) for a description of the methods. No, there's isn't one best method for all problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c511668cc6b32901ccd233c4164cc0e",
     "grade": false,
     "grade_id": "cell-ee75c910cc1dccc4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Your Turn\n",
    "\n",
    "Use `optimize.minimize` to find the minimum of the function\n",
    "\n",
    "$$f(x,y,z) = x \\cos(x) \\sin(y) \\sin(z)$$\n",
    "\n",
    "that is closest to $(1,1,1)$. Save the result in variables named `x`, `y`, and `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41499a63cc533e9b0ccc5c33cf4054a5",
     "grade": false,
     "grade_id": "cell-c85222addd195fd0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%%graded # 3 points\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45f5356ac2a9523e5fcf840a59be4f3b",
     "grade": true,
     "grade_id": "cell-66bb15c80b0dc98c",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%%tests\n",
    "\n",
    "_test.code_contains(\"minimize\")\n",
    "_test.similar(x, 3.43)\n",
    "_test.similar(y, -1.57)\n",
    "_test.similar(z, -1.57)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c183c6a4b42cb1cda63eaaf568a12ab4",
     "grade": false,
     "grade_id": "cell-1ae60099cab3bd92",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Additional Resources\n",
    "\n",
    "- Textbook section 6.4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
